{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,remove=('headers', 'footers', 'quotes'))\n",
    "data = dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def textPrecessing(text):\n",
    "    #小写化\n",
    "    text = text.lower()\n",
    "    #去除特殊标点\n",
    "    for c in string.punctuation:\n",
    "        text = text.replace(c, ' ')\n",
    "    #分词\n",
    "    wordLst = nltk.word_tokenize(text)\n",
    "    #去除停用词\n",
    "    filtered = [w for w in wordLst if w not in stopwords.words('english')]\n",
    "    #仅保留名词或特定POS   \n",
    "    refiltered =nltk.pos_tag(filtered)\n",
    "    filtered = [w for w, pos in refiltered if pos.startswith('NN')]\n",
    "\n",
    "    return \" \".join(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#文本预处理\n",
    "docLst = []\n",
    "for desc in data :\n",
    "    docLst.append(textPrecessing(desc).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 11314, n_features: 2500\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#构建词频向量\n",
    "vectorizer = CountVectorizer(max_features=2500,stop_words='english')\n",
    "X = vectorizer.fit_transform(docLst)\n",
    "print(\"n_samples: %d, n_features: %d\" % X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
       "       n_clusters=20, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmean = KMeans(n_clusters=5, max_iter=100)\n",
    "kmean.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: \n",
      " people  \n",
      " time  \n",
      " way  \n",
      " use  \n",
      " problem  \n",
      " edu  \n",
      " year  \n",
      " years  \n",
      " god  \n",
      " space  \n",
      "Cluster 1: \n",
      " ax  \n",
      " max  \n",
      " pl  \n",
      " bhj  \n",
      " giz  \n",
      " gk  \n",
      " qax  \n",
      " bj  \n",
      " lj  \n",
      " z5  \n",
      "Cluster 2: \n",
      " ax  \n",
      " max  \n",
      " b8f  \n",
      " a86  \n",
      " g9v  \n",
      " giz  \n",
      " bhj  \n",
      " pl  \n",
      " bxn  \n",
      " wm  \n",
      "Cluster 3: \n",
      " ax  \n",
      " max  \n",
      " sl  \n",
      " g9v  \n",
      " giz  \n",
      " ql  \n",
      " okz  \n",
      " chz  \n",
      " fyn  \n",
      " pl  \n",
      "Cluster 4: \n",
      " ax  \n",
      " max  \n",
      " b8f  \n",
      " bhj  \n",
      " bh  \n",
      " a86  \n",
      " g9v  \n",
      " air  \n",
      " ah  \n",
      " q45  \n",
      "Cluster 5: \n",
      " ax  \n",
      " max  \n",
      " pl  \n",
      " b8f  \n",
      " bhj  \n",
      " giz  \n",
      " a86  \n",
      " g9v  \n",
      " p2  \n",
      " r8f  \n",
      "Cluster 6: \n",
      " ax  \n",
      " max  \n",
      " g9v  \n",
      " a86  \n",
      " b8f  \n",
      " bhj  \n",
      " pl  \n",
      " mg9v  \n",
      " giz  \n",
      " gk  \n",
      "Cluster 7: \n",
      " ax  \n",
      " max  \n",
      " a86  \n",
      " b8f  \n",
      " pl  \n",
      " bhj  \n",
      " qax  \n",
      " qq  \n",
      " giz  \n",
      " i4  \n",
      "Cluster 8: \n",
      " ax  \n",
      " max  \n",
      " pl  \n",
      " b8f  \n",
      " g9v  \n",
      " wm  \n",
      " bxn  \n",
      " bhj  \n",
      " giz  \n",
      " qax  \n",
      "Cluster 9: \n",
      " cx  \n",
      " scx  \n",
      " sc  \n",
      " s6  \n",
      " ck  \n",
      " gc  \n",
      " gcx  \n",
      " hz  \n",
      " chz  \n",
      " lk  \n",
      "Cluster 10: \n",
      " ax  \n",
      " max  \n",
      " a86  \n",
      " b8f  \n",
      " pl  \n",
      " giz  \n",
      " ql  \n",
      " bhj  \n",
      " g9v  \n",
      " cx  \n",
      "Cluster 11: \n",
      " w7  \n",
      " cx  \n",
      " t7  \n",
      " a7  \n",
      " ck  \n",
      " w1  \n",
      " hz  \n",
      " lk  \n",
      " chz  \n",
      " mv  \n",
      "Cluster 12: \n",
      " ax  \n",
      " g9v  \n",
      " b8f  \n",
      " a86  \n",
      " max  \n",
      " pl  \n",
      " wm  \n",
      " okz  \n",
      " sl  \n",
      " bxn  \n",
      "Cluster 13: \n",
      " jpeg  \n",
      " image  \n",
      " file  \n",
      " color  \n",
      " gif  \n",
      " images  \n",
      " quality  \n",
      " format  \n",
      " version  \n",
      " bit  \n",
      "Cluster 14: \n",
      " db  \n",
      " mov  \n",
      " bh  \n",
      " cs  \n",
      " si  \n",
      " al  \n",
      " bl  \n",
      " byte  \n",
      " bits  \n",
      " cx  \n",
      "Cluster 15: \n",
      " file  \n",
      " information  \n",
      " pub  \n",
      " widget  \n",
      " use  \n",
      " window  \n",
      " edu  \n",
      " version  \n",
      " privacy  \n",
      " mit  \n",
      "Cluster 16: \n",
      " stephanopoulos  \n",
      " mr  \n",
      " president  \n",
      " time  \n",
      " george  \n",
      " package  \n",
      " groups  \n",
      " jobs  \n",
      " press  \n",
      " tax  \n",
      "Cluster 17: \n",
      " output  \n",
      " file  \n",
      " entry  \n",
      " program  \n",
      " stream  \n",
      " printf  \n",
      " char  \n",
      " line  \n",
      " check  \n",
      " fprintf  \n",
      "Cluster 18: \n",
      " image  \n",
      " graphics  \n",
      " data  \n",
      " pub  \n",
      " edu  \n",
      " software  \n",
      " mail  \n",
      " images  \n",
      " package  \n",
      " ftp  \n",
      "Cluster 19: \n",
      " ah  \n",
      " pl  \n",
      " air  \n",
      " chz  \n",
      " b8  \n",
      " b9r  \n",
      " w7  \n",
      " tl  \n",
      " pu  \n",
      " ahf  \n",
      "(20, 2500)\n"
     ]
    }
   ],
   "source": [
    "order_centroids = kmean.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(n_clusters):\n",
    "    print(\"Cluster %d: \" % i)\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s  ' % terms[ind])\n",
    "print(kmean.cluster_centers_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='batch', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=100,\n",
       "                          mean_change_tol=0.001, n_components=20, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=None,\n",
       "                          topic_word_prior=None, total_samples=1000000.0,\n",
       "                          verbose=0)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=20, max_iter=100)\n",
    "lda.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "power\n",
      "time\n",
      "use\n",
      "water\n",
      "ground\n",
      "way\n",
      "wire\n",
      "unit\n",
      "work\n",
      "line\n",
      "Topic #1:\n",
      "file\n",
      "files\n",
      "image\n",
      "window\n",
      "program\n",
      "version\n",
      "use\n",
      "code\n",
      "server\n",
      "pub\n",
      "Topic #2:\n",
      "president\n",
      "output\n",
      "program\n",
      "entry\n",
      "file\n",
      "mr\n",
      "stephanopoulos\n",
      "jobs\n",
      "line\n",
      "rules\n",
      "Topic #3:\n",
      "game\n",
      "team\n",
      "year\n",
      "games\n",
      "season\n",
      "players\n",
      "league\n",
      "play\n",
      "player\n",
      "hockey\n",
      "Topic #4:\n",
      "people\n",
      "time\n",
      "way\n",
      "day\n",
      "things\n",
      "thing\n",
      "days\n",
      "years\n",
      "home\n",
      "man\n",
      "Topic #5:\n",
      "evidence\n",
      "point\n",
      "question\n",
      "time\n",
      "fact\n",
      "argument\n",
      "example\n",
      "people\n",
      "book\n",
      "case\n",
      "Topic #6:\n",
      "war\n",
      "jews\n",
      "armenians\n",
      "people\n",
      "government\n",
      "israel\n",
      "world\n",
      "history\n",
      "turks\n",
      "population\n",
      "Topic #7:\n",
      "drive\n",
      "disk\n",
      "drives\n",
      "controller\n",
      "scsi\n",
      "bus\n",
      "card\n",
      "bios\n",
      "tape\n",
      "data\n",
      "Topic #8:\n",
      "thanks\n",
      "mail\n",
      "price\n",
      "sale\n",
      "offer\n",
      "list\n",
      "hi\n",
      "advance\n",
      "post\n",
      "information\n",
      "Topic #9:\n",
      "db\n",
      "bike\n",
      "vs\n",
      "van\n",
      "pts\n",
      "mov\n",
      "dod\n",
      "la\n",
      "cs\n",
      "bh\n",
      "Topic #10:\n",
      "cx\n",
      "chz\n",
      "ah\n",
      "w7\n",
      "lk\n",
      "hz\n",
      "mv\n",
      "scx\n",
      "t7\n",
      "ck\n",
      "Topic #11:\n",
      "gun\n",
      "state\n",
      "law\n",
      "government\n",
      "states\n",
      "crime\n",
      "rights\n",
      "control\n",
      "guns\n",
      "police\n",
      "Topic #12:\n",
      "chip\n",
      "encryption\n",
      "government\n",
      "keys\n",
      "security\n",
      "key\n",
      "privacy\n",
      "information\n",
      "use\n",
      "law\n",
      "Topic #13:\n",
      "software\n",
      "windows\n",
      "pc\n",
      "card\n",
      "problem\n",
      "memory\n",
      "apple\n",
      "use\n",
      "mac\n",
      "bit\n",
      "Topic #14:\n",
      "god\n",
      "jesus\n",
      "people\n",
      "life\n",
      "church\n",
      "christians\n",
      "religion\n",
      "man\n",
      "christ\n",
      "way\n",
      "Topic #15:\n",
      "people\n",
      "group\n",
      "work\n",
      "time\n",
      "years\n",
      "world\n",
      "question\n",
      "groups\n",
      "way\n",
      "lot\n",
      "Topic #16:\n",
      "edu\n",
      "com\n",
      "mail\n",
      "cs\n",
      "internet\n",
      "news\n",
      "address\n",
      "email\n",
      "university\n",
      "list\n",
      "Topic #17:\n",
      "car\n",
      "health\n",
      "cars\n",
      "engine\n",
      "year\n",
      "insurance\n",
      "problem\n",
      "patients\n",
      "years\n",
      "oil\n",
      "Topic #18:\n",
      "space\n",
      "data\n",
      "research\n",
      "center\n",
      "university\n",
      "information\n",
      "program\n",
      "science\n",
      "systems\n",
      "technology\n",
      "Topic #19:\n",
      "ax\n",
      "max\n",
      "g9v\n",
      "b8f\n",
      "a86\n",
      "pl\n",
      "giz\n",
      "bhj\n",
      "wm\n",
      "bxn\n"
     ]
    }
   ],
   "source": [
    "def print_top_words(model,feature_names,n_top_words):\n",
    "    # 打印每个主题下权重较高的term\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\"\\n\".join([feature_names[i] \n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "\n",
    "\n",
    "n_top_words=10\n",
    "tf_feature_names = vectorizer.get_feature_names()\n",
    "print_top_words(lda,tf_feature_names,n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class KMeans in module sklearn.cluster._kmeans:\n",
      "\n",
      "class KMeans(sklearn.base.TransformerMixin, sklearn.base.ClusterMixin, sklearn.base.BaseEstimator)\n",
      " |  KMeans(n_clusters=8, init='k-means++', n_init=10, max_iter=300, tol=0.0001, precompute_distances='auto', verbose=0, random_state=None, copy_x=True, n_jobs=None, algorithm='auto')\n",
      " |  \n",
      " |  K-Means clustering.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <k_means>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  \n",
      " |  n_clusters : int, default=8\n",
      " |      The number of clusters to form as well as the number of\n",
      " |      centroids to generate.\n",
      " |  \n",
      " |  init : {'k-means++', 'random'} or ndarray of shape             (n_clusters, n_features), default='k-means++'\n",
      " |      Method for initialization, defaults to 'k-means++':\n",
      " |  \n",
      " |      'k-means++' : selects initial cluster centers for k-mean\n",
      " |      clustering in a smart way to speed up convergence. See section\n",
      " |      Notes in k_init for more details.\n",
      " |  \n",
      " |      'random': choose k observations (rows) at random from data for\n",
      " |      the initial centroids.\n",
      " |  \n",
      " |      If an ndarray is passed, it should be of shape (n_clusters, n_features)\n",
      " |      and gives the initial centers.\n",
      " |  \n",
      " |  n_init : int, default=10\n",
      " |      Number of time the k-means algorithm will be run with different\n",
      " |      centroid seeds. The final results will be the best output of\n",
      " |      n_init consecutive runs in terms of inertia.\n",
      " |  \n",
      " |  max_iter : int, default=300\n",
      " |      Maximum number of iterations of the k-means algorithm for a\n",
      " |      single run.\n",
      " |  \n",
      " |  tol : float, default=1e-4\n",
      " |      Relative tolerance with regards to inertia to declare convergence.\n",
      " |  \n",
      " |  precompute_distances : 'auto' or bool, default='auto'\n",
      " |      Precompute distances (faster but takes more memory).\n",
      " |  \n",
      " |      'auto' : do not precompute distances if n_samples * n_clusters > 12\n",
      " |      million. This corresponds to about 100MB overhead per job using\n",
      " |      double precision.\n",
      " |  \n",
      " |      True : always precompute distances.\n",
      " |  \n",
      " |      False : never precompute distances.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      Verbosity mode.\n",
      " |  \n",
      " |  random_state : int, RandomState instance, default=None\n",
      " |      Determines random number generation for centroid initialization. Use\n",
      " |      an int to make the randomness deterministic.\n",
      " |      See :term:`Glossary <random_state>`.\n",
      " |  \n",
      " |  copy_x : bool, default=True\n",
      " |      When pre-computing distances it is more numerically accurate to center\n",
      " |      the data first.  If copy_x is True (default), then the original data is\n",
      " |      not modified, ensuring X is C-contiguous.  If False, the original data\n",
      " |      is modified, and put back before the function returns, but small\n",
      " |      numerical differences may be introduced by subtracting and then adding\n",
      " |      the data mean, in this case it will also not ensure that data is\n",
      " |      C-contiguous which may cause a significant slowdown.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      The number of jobs to use for the computation. This works by computing\n",
      " |      each of the n_init runs in parallel.\n",
      " |  \n",
      " |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      " |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      " |      for more details.\n",
      " |  \n",
      " |  algorithm : {\"auto\", \"full\", \"elkan\"}, default=\"auto\"\n",
      " |      K-means algorithm to use. The classical EM-style algorithm is \"full\".\n",
      " |      The \"elkan\" variation is more efficient by using the triangle\n",
      " |      inequality, but currently doesn't support sparse data. \"auto\" chooses\n",
      " |      \"elkan\" for dense data and \"full\" for sparse data.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  cluster_centers_ : ndarray of shape (n_clusters, n_features)\n",
      " |      Coordinates of cluster centers. If the algorithm stops before fully\n",
      " |      converging (see ``tol`` and ``max_iter``), these will not be\n",
      " |      consistent with ``labels_``.\n",
      " |  \n",
      " |  labels_ : ndarray of shape (n_samples,)\n",
      " |      Labels of each point\n",
      " |  \n",
      " |  inertia_ : float\n",
      " |      Sum of squared distances of samples to their closest cluster center.\n",
      " |  \n",
      " |  n_iter_ : int\n",
      " |      Number of iterations run.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  \n",
      " |  MiniBatchKMeans\n",
      " |      Alternative online implementation that does incremental updates\n",
      " |      of the centers positions using mini-batches.\n",
      " |      For large scale learning (say n_samples > 10k) MiniBatchKMeans is\n",
      " |      probably much faster than the default batch implementation.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The k-means problem is solved using either Lloyd's or Elkan's algorithm.\n",
      " |  \n",
      " |  The average complexity is given by O(k n T), were n is the number of\n",
      " |  samples and T is the number of iteration.\n",
      " |  \n",
      " |  The worst case complexity is given by O(n^(k+2/p)) with\n",
      " |  n = n_samples, p = n_features. (D. Arthur and S. Vassilvitskii,\n",
      " |  'How slow is the k-means method?' SoCG2006)\n",
      " |  \n",
      " |  In practice, the k-means algorithm is very fast (one of the fastest\n",
      " |  clustering algorithms available), but it falls in local minima. That's why\n",
      " |  it can be useful to restart it several times.\n",
      " |  \n",
      " |  If the algorithm stops before fully converging (because of ``tol`` or\n",
      " |  ``max_iter``), ``labels_`` and ``cluster_centers_`` will not be consistent,\n",
      " |  i.e. the ``cluster_centers_`` will not be the means of the points in each\n",
      " |  cluster. Also, the estimator will reassign ``labels_`` after the last\n",
      " |  iteration to make ``labels_`` consistent with ``predict`` on the training\n",
      " |  set.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  \n",
      " |  >>> from sklearn.cluster import KMeans\n",
      " |  >>> import numpy as np\n",
      " |  >>> X = np.array([[1, 2], [1, 4], [1, 0],\n",
      " |  ...               [10, 2], [10, 4], [10, 0]])\n",
      " |  >>> kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
      " |  >>> kmeans.labels_\n",
      " |  array([1, 1, 1, 0, 0, 0], dtype=int32)\n",
      " |  >>> kmeans.predict([[0, 0], [12, 3]])\n",
      " |  array([1, 0], dtype=int32)\n",
      " |  >>> kmeans.cluster_centers_\n",
      " |  array([[10.,  2.],\n",
      " |         [ 1.,  2.]])\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      KMeans\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.base.ClusterMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_clusters=8, init='k-means++', n_init=10, max_iter=300, tol=0.0001, precompute_distances='auto', verbose=0, random_state=None, copy_x=True, n_jobs=None, algorithm='auto')\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None, sample_weight=None)\n",
      " |      Compute k-means clustering.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape=(n_samples, n_features)\n",
      " |          Training instances to cluster. It must be noted that the data\n",
      " |          will be converted to C ordering, which will cause a memory\n",
      " |          copy if the given data is not C-contiguous.\n",
      " |      \n",
      " |      y : Ignored\n",
      " |          Not used, present here for API consistency by convention.\n",
      " |      \n",
      " |      sample_weight : array-like, shape (n_samples,), optional\n",
      " |          The weights for each observation in X. If None, all observations\n",
      " |          are assigned equal weight (default: None).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  fit_predict(self, X, y=None, sample_weight=None)\n",
      " |      Compute cluster centers and predict cluster index for each sample.\n",
      " |      \n",
      " |      Convenience method; equivalent to calling fit(X) followed by\n",
      " |      predict(X).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          New data to transform.\n",
      " |      \n",
      " |      y : Ignored\n",
      " |          Not used, present here for API consistency by convention.\n",
      " |      \n",
      " |      sample_weight : array-like, shape (n_samples,), optional\n",
      " |          The weights for each observation in X. If None, all observations\n",
      " |          are assigned equal weight (default: None).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      labels : array, shape [n_samples,]\n",
      " |          Index of the cluster each sample belongs to.\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, sample_weight=None)\n",
      " |      Compute clustering and transform X to cluster-distance space.\n",
      " |      \n",
      " |      Equivalent to fit(X).transform(X), but more efficiently implemented.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          New data to transform.\n",
      " |      \n",
      " |      y : Ignored\n",
      " |          Not used, present here for API consistency by convention.\n",
      " |      \n",
      " |      sample_weight : array-like, shape (n_samples,), optional\n",
      " |          The weights for each observation in X. If None, all observations\n",
      " |          are assigned equal weight (default: None).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : array, shape [n_samples, k]\n",
      " |          X transformed in the new space.\n",
      " |  \n",
      " |  predict(self, X, sample_weight=None)\n",
      " |      Predict the closest cluster each sample in X belongs to.\n",
      " |      \n",
      " |      In the vector quantization literature, `cluster_centers_` is called\n",
      " |      the code book and each value returned by `predict` is the index of\n",
      " |      the closest code in the code book.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          New data to predict.\n",
      " |      \n",
      " |      sample_weight : array-like, shape (n_samples,), optional\n",
      " |          The weights for each observation in X. If None, all observations\n",
      " |          are assigned equal weight (default: None).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      labels : array, shape [n_samples,]\n",
      " |          Index of the cluster each sample belongs to.\n",
      " |  \n",
      " |  score(self, X, y=None, sample_weight=None)\n",
      " |      Opposite of the value of X on the K-means objective.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          New data.\n",
      " |      \n",
      " |      y : Ignored\n",
      " |          Not used, present here for API consistency by convention.\n",
      " |      \n",
      " |      sample_weight : array-like, shape (n_samples,), optional\n",
      " |          The weights for each observation in X. If None, all observations\n",
      " |          are assigned equal weight (default: None).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Opposite of the value of X on the K-means objective.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Transform X to a cluster-distance space.\n",
      " |      \n",
      " |      In the new space, each dimension is the distance to the cluster\n",
      " |      centers.  Note that even if X is sparse, the array returned by\n",
      " |      `transform` will typically be dense.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          New data to transform.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : array, shape [n_samples, k]\n",
      " |          X transformed in the new space.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(KMeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LatentDirichletAllocation in module sklearn.decomposition._lda:\n",
      "\n",
      "class LatentDirichletAllocation(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      " |  LatentDirichletAllocation(n_components=10, doc_topic_prior=None, topic_word_prior=None, learning_method='batch', learning_decay=0.7, learning_offset=10.0, max_iter=10, batch_size=128, evaluate_every=-1, total_samples=1000000.0, perp_tol=0.1, mean_change_tol=0.001, max_doc_update_iter=100, n_jobs=None, verbose=0, random_state=None)\n",
      " |  \n",
      " |  Latent Dirichlet Allocation with online variational Bayes algorithm\n",
      " |  \n",
      " |  .. versionadded:: 0.17\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <LatentDirichletAllocation>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_components : int, optional (default=10)\n",
      " |      Number of topics.\n",
      " |  \n",
      " |  doc_topic_prior : float, optional (default=None)\n",
      " |      Prior of document topic distribution `theta`. If the value is None,\n",
      " |      defaults to `1 / n_components`.\n",
      " |      In [1]_, this is called `alpha`.\n",
      " |  \n",
      " |  topic_word_prior : float, optional (default=None)\n",
      " |      Prior of topic word distribution `beta`. If the value is None, defaults\n",
      " |      to `1 / n_components`.\n",
      " |      In [1]_, this is called `eta`.\n",
      " |  \n",
      " |  learning_method : 'batch' | 'online', default='batch'\n",
      " |      Method used to update `_component`. Only used in :meth:`fit` method.\n",
      " |      In general, if the data size is large, the online update will be much\n",
      " |      faster than the batch update.\n",
      " |  \n",
      " |      Valid options::\n",
      " |  \n",
      " |          'batch': Batch variational Bayes method. Use all training data in\n",
      " |              each EM update.\n",
      " |              Old `components_` will be overwritten in each iteration.\n",
      " |          'online': Online variational Bayes method. In each EM update, use\n",
      " |              mini-batch of training data to update the ``components_``\n",
      " |              variable incrementally. The learning rate is controlled by the\n",
      " |              ``learning_decay`` and the ``learning_offset`` parameters.\n",
      " |  \n",
      " |      .. versionchanged:: 0.20\n",
      " |          The default learning method is now ``\"batch\"``.\n",
      " |  \n",
      " |  learning_decay : float, optional (default=0.7)\n",
      " |      It is a parameter that control learning rate in the online learning\n",
      " |      method. The value should be set between (0.5, 1.0] to guarantee\n",
      " |      asymptotic convergence. When the value is 0.0 and batch_size is\n",
      " |      ``n_samples``, the update method is same as batch learning. In the\n",
      " |      literature, this is called kappa.\n",
      " |  \n",
      " |  learning_offset : float, optional (default=10.)\n",
      " |      A (positive) parameter that downweights early iterations in online\n",
      " |      learning.  It should be greater than 1.0. In the literature, this is\n",
      " |      called tau_0.\n",
      " |  \n",
      " |  max_iter : integer, optional (default=10)\n",
      " |      The maximum number of iterations.\n",
      " |  \n",
      " |  batch_size : int, optional (default=128)\n",
      " |      Number of documents to use in each EM iteration. Only used in online\n",
      " |      learning.\n",
      " |  \n",
      " |  evaluate_every : int, optional (default=0)\n",
      " |      How often to evaluate perplexity. Only used in `fit` method.\n",
      " |      set it to 0 or negative number to not evaluate perplexity in\n",
      " |      training at all. Evaluating perplexity can help you check convergence\n",
      " |      in training process, but it will also increase total training time.\n",
      " |      Evaluating perplexity in every iteration might increase training time\n",
      " |      up to two-fold.\n",
      " |  \n",
      " |  total_samples : int, optional (default=1e6)\n",
      " |      Total number of documents. Only used in the :meth:`partial_fit` method.\n",
      " |  \n",
      " |  perp_tol : float, optional (default=1e-1)\n",
      " |      Perplexity tolerance in batch learning. Only used when\n",
      " |      ``evaluate_every`` is greater than 0.\n",
      " |  \n",
      " |  mean_change_tol : float, optional (default=1e-3)\n",
      " |      Stopping tolerance for updating document topic distribution in E-step.\n",
      " |  \n",
      " |  max_doc_update_iter : int (default=100)\n",
      " |      Max number of iterations for updating document topic distribution in\n",
      " |      the E-step.\n",
      " |  \n",
      " |  n_jobs : int or None, optional (default=None)\n",
      " |      The number of jobs to use in the E-step.\n",
      " |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      " |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      " |      for more details.\n",
      " |  \n",
      " |  verbose : int, optional (default=0)\n",
      " |      Verbosity level.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional (default=None)\n",
      " |      If int, random_state is the seed used by the random number generator;\n",
      " |      If RandomState instance, random_state is the random number generator;\n",
      " |      If None, the random number generator is the RandomState instance used\n",
      " |      by `np.random`.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  components_ : array, [n_components, n_features]\n",
      " |      Variational parameters for topic word distribution. Since the complete\n",
      " |      conditional for topic word distribution is a Dirichlet,\n",
      " |      ``components_[i, j]`` can be viewed as pseudocount that represents the\n",
      " |      number of times word `j` was assigned to topic `i`.\n",
      " |      It can also be viewed as distribution over the words for each topic\n",
      " |      after normalization:\n",
      " |      ``model.components_ / model.components_.sum(axis=1)[:, np.newaxis]``.\n",
      " |  \n",
      " |  n_batch_iter_ : int\n",
      " |      Number of iterations of the EM step.\n",
      " |  \n",
      " |  n_iter_ : int\n",
      " |      Number of passes over the dataset.\n",
      " |  \n",
      " |  bound_ : float\n",
      " |      Final perplexity score on training set.\n",
      " |  \n",
      " |  doc_topic_prior_ : float\n",
      " |      Prior of document topic distribution `theta`. If the value is None,\n",
      " |      it is `1 / n_components`.\n",
      " |  \n",
      " |  topic_word_prior_ : float\n",
      " |      Prior of topic word distribution `beta`. If the value is None, it is\n",
      " |      `1 / n_components`.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.decomposition import LatentDirichletAllocation\n",
      " |  >>> from sklearn.datasets import make_multilabel_classification\n",
      " |  >>> # This produces a feature matrix of token counts, similar to what\n",
      " |  >>> # CountVectorizer would produce on text.\n",
      " |  >>> X, _ = make_multilabel_classification(random_state=0)\n",
      " |  >>> lda = LatentDirichletAllocation(n_components=5,\n",
      " |  ...     random_state=0)\n",
      " |  >>> lda.fit(X)\n",
      " |  LatentDirichletAllocation(...)\n",
      " |  >>> # get topics for some given samples:\n",
      " |  >>> lda.transform(X[-2:])\n",
      " |  array([[0.00360392, 0.25499205, 0.0036211 , 0.64236448, 0.09541846],\n",
      " |         [0.15297572, 0.00362644, 0.44412786, 0.39568399, 0.003586  ]])\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  .. [1] \"Online Learning for Latent Dirichlet Allocation\", Matthew D.\n",
      " |      Hoffman, David M. Blei, Francis Bach, 2010\n",
      " |  \n",
      " |  [2] \"Stochastic Variational Inference\", Matthew D. Hoffman, David M. Blei,\n",
      " |      Chong Wang, John Paisley, 2013\n",
      " |  \n",
      " |  [3] Matthew D. Hoffman's onlineldavb code. Link:\n",
      " |      https://github.com/blei-lab/onlineldavb\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LatentDirichletAllocation\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_components=10, doc_topic_prior=None, topic_word_prior=None, learning_method='batch', learning_decay=0.7, learning_offset=10.0, max_iter=10, batch_size=128, evaluate_every=-1, total_samples=1000000.0, perp_tol=0.1, mean_change_tol=0.001, max_doc_update_iter=100, n_jobs=None, verbose=0, random_state=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Learn model for the data X with variational Bayes method.\n",
      " |      \n",
      " |      When `learning_method` is 'online', use mini-batch update.\n",
      " |      Otherwise, use batch update.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape=(n_samples, n_features)\n",
      " |          Document word matrix.\n",
      " |      \n",
      " |      y : Ignored\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  partial_fit(self, X, y=None)\n",
      " |      Online VB with Mini-Batch update.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape=(n_samples, n_features)\n",
      " |          Document word matrix.\n",
      " |      \n",
      " |      y : Ignored\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  perplexity(self, X, sub_sampling=False)\n",
      " |      Calculate approximate perplexity for data X.\n",
      " |      \n",
      " |      Perplexity is defined as exp(-1. * log-likelihood per word)\n",
      " |      \n",
      " |      .. versionchanged:: 0.19\n",
      " |         *doc_topic_distr* argument has been deprecated and is ignored\n",
      " |         because user no longer has access to unnormalized distribution\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, [n_samples, n_features]\n",
      " |          Document word matrix.\n",
      " |      \n",
      " |      sub_sampling : bool\n",
      " |          Do sub-sampling or not.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Perplexity score.\n",
      " |  \n",
      " |  score(self, X, y=None)\n",
      " |      Calculate approximate log-likelihood as score.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape=(n_samples, n_features)\n",
      " |          Document word matrix.\n",
      " |      \n",
      " |      y : Ignored\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Use approximate bound as score.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Transform data X according to the fitted model.\n",
      " |      \n",
      " |         .. versionchanged:: 0.18\n",
      " |            *doc_topic_distr* is now normalized\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape=(n_samples, n_features)\n",
      " |          Document word matrix.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      doc_topic_distr : shape=(n_samples, n_components)\n",
      " |          Document topic distribution for X.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |      \n",
      " |      Fits transformer to X and y with optional parameters fit_params\n",
      " |      and returns a transformed version of X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : numpy array of shape [n_samples, n_features]\n",
      " |          Training set.\n",
      " |      \n",
      " |      y : numpy array of shape [n_samples]\n",
      " |          Target values.\n",
      " |      \n",
      " |      **fit_params : dict\n",
      " |          Additional fit parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : numpy array of shape [n_samples, n_features_new]\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(LatentDirichletAllocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
