{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corpus = [\n",
    "    'Pumas are large, cat-like animals which are found in America. When reports came into London Zoo that a wild puma had been spotted forty-five miles south of London, they were not taken seriously.',\n",
    "    'However, as the evidence began to accumulate, experts from the Zoo felt obliged to investigate, for the descriptions given by people who claimed to have seen the puma were extraordinarily similar.',\n",
    "    'The hunt for the puma began in a small village where a woman picking blackberries saw a large cat only five yards away from her. It immediately ran away when she saw it, and experts confirmed that a puma will not attack a human being unless it is cornered.',\n",
    "    'The search proved difficult, for the puma was often observed at one place in the morning and at another place twenty miles away in the evening. Wherever it went, it left behind it a trail of dead deer and small animals like rabbits.',\n",
    "    'Paw prints were seen in a number of places and puma fur was found clinging to bushes. Several people complained of \"cat-like noises at night and a businessman on a fishing trip saw the puma up a tree.',\n",
    "    'The experts were now fully convinced that the animal was a puma, but where had it come from? As no pumas had been reported missing from any zoo in the country, this one must have been in the possession of a private collector and somehow managed to escape.',\n",
    "    'The hunt went on for several weeks, but the puma was not caught. It is disturbing to think that a dangerous wild animal is still at large in the quiet countryside.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "tfidf_vec = TfidfVectorizer() \n",
    "tfidf_matrix = tfidf_vec.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accumulate', 'america', 'and', 'animal', 'animals', 'another', 'any', 'are', 'as', 'at', 'attack', 'away', 'been', 'began', 'behind', 'being', 'blackberries', 'bushes', 'businessman', 'but', 'by', 'came', 'cat', 'caught', 'claimed', 'clinging', 'collector', 'come', 'complained', 'confirmed', 'convinced', 'cornered', 'country', 'countryside', 'dangerous', 'dead', 'deer', 'descriptions', 'difficult', 'disturbing', 'escape', 'evening', 'evidence', 'experts', 'extraordinarily', 'felt', 'fishing', 'five', 'for', 'forty', 'found', 'from', 'fully', 'fur', 'given', 'had', 'have', 'her', 'however', 'human', 'hunt', 'immediately', 'in', 'into', 'investigate', 'is', 'it', 'large', 'left', 'like', 'london', 'managed', 'miles', 'missing', 'morning', 'must', 'night', 'no', 'noises', 'not', 'now', 'number', 'obliged', 'observed', 'of', 'often', 'on', 'one', 'only', 'paw', 'people', 'picking', 'place', 'places', 'possession', 'prints', 'private', 'proved', 'puma', 'pumas', 'quiet', 'rabbits', 'ran', 'reported', 'reports', 'saw', 'search', 'seen', 'seriously', 'several', 'she', 'similar', 'small', 'somehow', 'south', 'spotted', 'still', 'taken', 'that', 'the', 'they', 'think', 'this', 'to', 'trail', 'tree', 'trip', 'twenty', 'unless', 'up', 'village', 'was', 'weeks', 'went', 'were', 'when', 'where', 'wherever', 'which', 'who', 'wild', 'will', 'woman', 'yards', 'zoo']\n",
      "{'pumas': 99, 'are': 7, 'large': 67, 'cat': 22, 'like': 69, 'animals': 4, 'which': 138, 'found': 50, 'in': 62, 'america': 1, 'when': 135, 'reports': 104, 'came': 21, 'into': 63, 'london': 70, 'zoo': 144, 'that': 118, 'wild': 140, 'puma': 98, 'had': 55, 'been': 12, 'spotted': 115, 'forty': 49, 'five': 47, 'miles': 72, 'south': 114, 'of': 84, 'they': 120, 'were': 134, 'not': 79, 'taken': 117, 'seriously': 108, 'however': 58, 'as': 8, 'the': 119, 'evidence': 42, 'began': 13, 'to': 123, 'accumulate': 0, 'experts': 43, 'from': 51, 'felt': 45, 'obliged': 82, 'investigate': 64, 'for': 48, 'descriptions': 37, 'given': 54, 'by': 20, 'people': 90, 'who': 139, 'claimed': 24, 'have': 56, 'seen': 107, 'extraordinarily': 44, 'similar': 111, 'hunt': 60, 'small': 112, 'village': 130, 'where': 136, 'woman': 142, 'picking': 91, 'blackberries': 16, 'saw': 105, 'only': 88, 'yards': 143, 'away': 11, 'her': 57, 'it': 66, 'immediately': 61, 'ran': 102, 'she': 110, 'and': 2, 'confirmed': 29, 'will': 141, 'attack': 10, 'human': 59, 'being': 15, 'unless': 128, 'is': 65, 'cornered': 31, 'search': 106, 'proved': 97, 'difficult': 38, 'was': 131, 'often': 85, 'observed': 83, 'at': 9, 'one': 87, 'place': 92, 'morning': 74, 'another': 5, 'twenty': 127, 'evening': 41, 'wherever': 137, 'went': 133, 'left': 68, 'behind': 14, 'trail': 124, 'dead': 35, 'deer': 36, 'rabbits': 101, 'paw': 89, 'prints': 95, 'number': 81, 'places': 93, 'fur': 53, 'clinging': 25, 'bushes': 17, 'several': 109, 'complained': 28, 'noises': 78, 'night': 76, 'businessman': 18, 'on': 86, 'fishing': 46, 'trip': 126, 'up': 129, 'tree': 125, 'now': 80, 'fully': 52, 'convinced': 30, 'animal': 3, 'but': 19, 'come': 27, 'no': 77, 'reported': 103, 'missing': 73, 'any': 6, 'country': 32, 'this': 122, 'must': 75, 'possession': 94, 'private': 96, 'collector': 26, 'somehow': 113, 'managed': 71, 'escape': 40, 'weeks': 132, 'caught': 23, 'disturbing': 39, 'think': 121, 'dangerous': 34, 'still': 116, 'quiet': 100, 'countryside': 33}\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_vec.get_feature_names())\n",
    "print(tfidf_vec.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.18487403 0.         ... 0.         0.         0.13117365]\n",
      " [0.19643497 0.         0.         ... 0.         0.         0.13937649]\n",
      " [0.         0.         0.1023333  ... 0.16612026 0.16612026 0.        ]\n",
      " ...\n",
      " [0.         0.         0.24175029 ... 0.         0.         0.        ]\n",
      " [0.         0.         0.10032698 ... 0.         0.         0.11555641]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'However, as the evidence began to accumulate, experts from the Zoo felt obliged to investigate, for the descriptions given by people who claimed to have seen the puma were extraordinarily similar.'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\DELL\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.679 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "关键词：\n",
      "began 出现的频率为:0.082775\n",
      "extraordinarily 出现的频率为:0.082775\n",
      "accumulate 出现的频率为:0.077769\n",
      "puma 出现的频率为:0.077769\n",
      "experts 出现的频率为:0.074998\n",
      "claimed 出现的频率为:0.074998\n",
      "zoo 出现的频率为:0.073488\n",
      "people 出现的频率为:0.073488\n",
      "felt 出现的频率为:0.072704\n",
      "descriptions 出现的频率为:0.072704\n"
     ]
    }
   ],
   "source": [
    "from textrank4zh import TextRank4Keyword, TextRank4Sentence \n",
    "text = corpus[1]\n",
    "tr4w = TextRank4Keyword()\n",
    "tr4w.analyze(text=text, lower=True, window=2)\n",
    "print( '关键词：' )\n",
    "for item in tr4w.get_keywords(10, word_min_len=1):\n",
    "    print(\"{} 出现的频率为:{:.6f}\".format(item.word, item.weight)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
       "       n_clusters=2, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmean = KMeans(n_clusters=2, max_iter=100)\n",
    "kmean.fit(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: \n",
      " amp  \n",
      " ac  \n",
      " apollo  \n",
      " aid  \n",
      " activity  \n",
      "Cluster 1: \n",
      " aspects  \n",
      " america  \n",
      " assistance  \n",
      " amendment  \n",
      " alternatives  \n",
      "(2, 145)\n"
     ]
    }
   ],
   "source": [
    "order_centroids = kmean.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(2):\n",
    "    print(\"Cluster %d: \" % i)\n",
    "    for ind in order_centroids[i, :5]:\n",
    "        print(' %s  ' % terms[ind])\n",
    "print(kmean.cluster_centers_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='batch', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=100,\n",
       "                          mean_change_tol=0.001, n_components=2, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=None,\n",
       "                          topic_word_prior=None, total_samples=1000000.0,\n",
       "                          verbose=0)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=2, max_iter=100)\n",
    "lda.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "ac\n",
      "amp\n",
      "assistance\n",
      "aspects\n",
      "author\n",
      "Topic #1:\n",
      "aspects\n",
      "america\n",
      "abortion\n",
      "alternatives\n",
      "arabs\n"
     ]
    }
   ],
   "source": [
    "def print_top_words(model,feature_names,n_top_words):\n",
    "    # 打印每个主题下权重较高的term\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\"\\n\".join([feature_names[i] \n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "\n",
    "\n",
    "n_top_words=5\n",
    "tf_feature_names = vectorizer.get_feature_names()\n",
    "print_top_words(lda,tf_feature_names,n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
